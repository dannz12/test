{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EE-361M Introduction to Data Mining\n",
    "## Assignment #3\n",
    "## Due: Thursday, Mar 4, 2016 by midnight; Total points: 50\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook** (if this isn't possible, let me know). Please use this naming format for your notebook you submit: **Group(Group Num)_HW(HW Number).ipynb**. For example, Group1_HW1.ipynb. Homeworks should be submitted through Canvas in your **groups of 3 from the first homework**. If groups need to be adjusted please contact the TA. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 1 (2+1 = 3 points)\n",
    "\n",
    "View the video at:\n",
    "\n",
    "https://www.youtube.com/watch?v=jbkSRLYSojo\n",
    "\n",
    "(Hans Rosling's 200 Countries, 200 Years, 4 Minutes) and answer the following questions:\n",
    "\n",
    "1. How many variables are being visualized in the “moving bubble plots” video (list them)?\n",
    "\n",
    "2. Identify a variable that is “zoomed into”, i.e., examined at a sub-category or more detailed level.\n",
    "\n",
    "\n",
    "FACTOID: Rosling’s gapminder visualization\n",
    "\n",
    "(see https://www.youtube.com/user/Gapcast for some more insightful videos) can now be\n",
    "\n",
    "readily used by you via Google Charts: https://developers.google.com/chart/interactive/docs/gallery\n",
    "\n",
    "Just plug in your own variables into “Bubble Chart” under the URL above and go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 4 expected life span, average income, population, region,\n",
    "2. The region variable is being zoomed into at times. The Regions are broken into countries to begin with and can be further broken down into provinces which could be broken down even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 2 (3+3+2+2=10 points)\n",
    "\n",
    "In this question, you will explore the application of Lasso and Ridge regression using sklearn package in Python. The dataset is prostate cancer data. The data can be found on canvas on the homework 3 page as prostate.csv. More information on the data can be found [here](https://cran.r-project.org/web/packages/ElemStatLearn/ElemStatLearn.pdf) under prostate. Use a random state of 42 and a test size of 1/3 to [split the data into training and test](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html). We will be using all the variables to predict lcavol. \n",
    "\n",
    "1. Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html). For the sweep of the regularization parameter lambda (Note: lambda is called alpha in sklearn), use [0.00001, 0.0001,0.001, 0.005, 0.01, 0.05, 0.1, 1, 5, 10, 100]  for ridge and [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5] for lasso. Report the best chosen based on cross-validation. The cross validation should happen on your training data using  average MSE as the scoring metric.\n",
    "2. Run ridge and lasso for all of the parameters specified above (on all training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot (e.g. Fig 6.6 of JW). What do you qualitatively observe when value of the regularization parameter is changed? \n",
    "3. Run least squares regression, ridge, and lasso on the full training data. For ridge and lasso, use only the best regularization parameter. Report the prediction error on the test data for each.\n",
    "4. For the best lasso parameter, determine the variables that were not dropped. Using only these variables, run least squares regression on full training data and report the prediction error on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split, KFold, cross_val_score\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "\n",
    "prostate_cancer_data_raw = pd.read_csv(\"C:\\prostate.csv\", quoting=1)\n",
    "prostate_cancer_data_cleaned = prostate_cancer_data_raw.dropna()\n",
    "#print(prostate_cancer_data_raw)\n",
    "\n",
    "prostate_cancer_train, prostate_cancer_test = train_test_split(prostate_cancer_data_cleaned, test_size=0.33, random_state=42)\n",
    "\n",
    "prostate_cancer_tindep = prostate_cancer_train.drop(['lcavol'], axis=1)\n",
    "prostate_cancer_tdep = prostate_cancer_train[['lcavol']]\n",
    "#print(prostate_cancer_tindep)\n",
    "#print(prostate_cancer_tdep)\n",
    "\n",
    "#scores are turned into positive to make MSE score\n",
    "\n",
    "#scores for lasso\n",
    "lasso = Lasso()\n",
    "lasso_alphas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "lasso_scores = list()\n",
    "lasso_scores_std = list()\n",
    "for las_alpha in lasso_alphas:\n",
    "    lasso.alpha = las_alpha\n",
    "    cur_scores = cross_val_score(lasso, prostate_cancer_tindep, prostate_cancer_tdep, scoring = 'mean_squared_error', cv = 5, n_jobs = 1)\n",
    "    cur_scores = abs(cur_scores)\n",
    "    #print(cur_scores)\n",
    "    lasso_scores.append(np.mean(cur_scores))\n",
    "    lasso_scores_std.append(np.std(cur_scores))\n",
    "\n",
    "#print(lasso_scores)\n",
    "\n",
    "#find best alpha for lasso\n",
    "lasso_idx = np.argmax(lasso_scores)\n",
    "print(\"Lasso Alpha:\", lasso_alphas[lasso_idx], \"Score:\", lasso_scores[lasso_idx])\n",
    "\n",
    "#scores for lasso\n",
    "ridge = Ridge()\n",
    "ridge_alphas = [0.00001, 0.0001,0.001, 0.005, 0.01, 0.05, 0.1, 1, 5, 10, 100]\n",
    "ridge_scores = list()\n",
    "ridge_scores_std = list()\n",
    "for rid_alpha in ridge_alphas:\n",
    "    ridge.alpha = rid_alpha\n",
    "    cur_scores = cross_val_score(ridge, prostate_cancer_tindep, prostate_cancer_tdep, scoring = 'mean_squared_error', cv = 5, n_jobs = 1)\n",
    "    cur_scores = abs(cur_scores)\n",
    "    #print(cur_scores)\n",
    "    ridge_scores.append(np.mean(cur_scores))\n",
    "    ridge_scores_std.append(np.std(cur_scores))\n",
    "\n",
    "#find best alpha for ridge (minimimum MSE)\n",
    "ridge_idx = np.argmin(ridge_scores)\n",
    "print(\"Ridge Alpha:\", ridge_alphas[ridge_idx], \"Score:\", ridge_scores[ridge_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "markers = ['1', '2', '3', '4', '8', 's', '*', 'o', '.', '+', 'x', 'D']\n",
    "\n",
    "fig = plt.figure(1)\n",
    "lplot = fig.add_subplot(211)\n",
    "rplot = fig.add_subplot(212)\n",
    "lplot.set_title(\"Lasso\")\n",
    "rplot.set_title(\"Ridge\")\n",
    "\n",
    "midx = 0\n",
    "for l_alpha in lasso_alphas:\n",
    "    lasso = Lasso(alpha=l_alpha)\n",
    "    lasso.fit(prostate_cancer_tindep, prostate_cancer_tdep)\n",
    "    l_coeff = lasso.coef_\n",
    "    lplot.plot(l_coeff, linestyle='--', marker=markers[midx], label=str(l_alpha))\n",
    "    midx += 1\n",
    "    \n",
    "midx = 0\n",
    "for r_alpha in ridge_alphas:\n",
    "    ridge = Ridge(alpha=r_alpha)\n",
    "    ridge.fit(prostate_cancer_tindep, prostate_cancer_tdep)\n",
    "    r_coeff = ridge.coef_[0]\n",
    "    rplot.plot(r_coeff, linestyle='--', marker=markers[midx], label=str(r_alpha))\n",
    "    midx += 1\n",
    "    \n",
    "lplot.legend(bbox_to_anchor=(1.05, 1), loc=2, ncol=3, borderaxespad=0.)\n",
    "rplot.legend(bbox_to_anchor=(1.05, 1), loc=2, ncol=3, borderaxespad=0.)\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the figure above, when the alpha value increase, the coefficients have less differences among them. In other words, coefficients vary less as alpha value increases. In addition, coefficient values remain relative same even if alpha value varied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#best regularization parameter from part 1\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(prostate_cancer_tindep, prostate_cancer_tdep)\n",
    "ridge = Ridge(alpha=10)\n",
    "ridge.fit(prostate_cancer_tindep, prostate_cancer_tdep)\n",
    "linear = LinearRegression()\n",
    "linear.fit(prostate_cancer_tindep, prostate_cancer_tdep)\n",
    "\n",
    "#prediction error from test data\n",
    "prostate_cancer_ttindep = prostate_cancer_test.drop(['lcavol'], axis=1)\n",
    "prostate_cancer_ttdep = prostate_cancer_test[['lcavol']]\n",
    "\n",
    "lasso_predicted = lasso.predict(prostate_cancer_ttindep)\n",
    "ridge_predicted = ridge.predict(prostate_cancer_ttindep)\n",
    "linear_predicted = linear.predict(prostate_cancer_ttindep)\n",
    "\n",
    "model_actual = prostate_cancer_ttdep\n",
    "\n",
    "lasso_mse = mean_squared_error(model_actual, lasso_predicted)\n",
    "ridge_mse = mean_squared_error(model_actual, ridge_predicted)\n",
    "linear_mse = mean_squared_error(model_actual, linear_predicted)\n",
    "\n",
    "#print results\n",
    "print(\"Lasso prediction error:\", lasso_mse)\n",
    "print(\"Ridge prediction error:\", ridge_mse)\n",
    "print(\"Linear prediction error:\", linear_mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the best lasso parameter, determine the variables that were not dropped. Using only these variables, run least squares regression on full training data and report the prediction error on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best lasso paramater is 0.5 as obtained in part 1\n",
    "lasso = Lasso(alpha=l_alpha)\n",
    "lasso.fit(prostate_cancer_tindep, prostate_cancer_tdep)\n",
    "l_coeff = lasso.coef_\n",
    "\n",
    "#determine variables (utilize non zero coefficients)\n",
    "idx = np.flatnonzero(l_coeff)\n",
    "prostate_cancer_tindepcrt = prostate_cancer_tindep.ix[:,idx]\n",
    "prostate_cancer_ttindepcrt = prostate_cancer_ttindep.ix[:,idx]\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(prostate_cancer_tindepcrt, prostate_cancer_tdep)\n",
    "linear_predicted = linear.predict(prostate_cancer_ttindepcrt)\n",
    "linear_mse = mean_squared_error(model_actual, linear_predicted)\n",
    "print(\"Linear prediction error with selective variables:\", linear_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (3+3+2+2 = 10 points)\n",
    "\n",
    "Re-solve all the questions in question 2 using R. You can submit the code and results via a PDF or other format. Just please make a reference to it in your notebook. See hints.R on the Canvas homework 3 page to help get you started. I would recommend using [RStudio](https://www.rstudio.com/products/rstudio/download/) for your work in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an attached fiile for this section titled HW3Q4DATAMINING.r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (5+5 = 10 points)\n",
    "\n",
    "1. Derive the coefficent updates, from first principles, for a gradient descent version of linear regression. Hint: start from the cost function. If you write the math by hand, submit that as a separate file and make a reference to it in your notebook or include the image in your notebook.\n",
    "2. Write Python code for a gradient descent version of linear regression. Should be similar to sklearn in that you have a fit function that takes an X, y, learning rate, and number of iterations and a predict funtion that takes an X value. Use your new SGD regression to re-run question 2.4 and compare MSE. Make sure you always normalize your X matrices and use an intercept. You can also compare your results with SGDRegressor from sklearn, but not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derivation is attached as derivation.jpg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pylab\n",
    "from scipy import stats\n",
    "from __future__ import division\n",
    "\n",
    "class GradientDescent:\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        # add bias term to input data\n",
    "        bias = np.ones(shape=(X.shape[0],1))\n",
    "        X_with_bias = np.concatenate((bias, X), axis=1)\n",
    "        return np.dot(X_with_bias, self.theta)\n",
    "        \n",
    "    def fit(self, x, y, alpha, num_iter):\n",
    "        # number of samples\n",
    "        m = x.shape[0]\n",
    "        \n",
    "        # add bias term to input data\n",
    "        bias = np.ones(shape=(x.shape[0],1))\n",
    "        x = np.concatenate((bias, x), axis=1)\n",
    "        \n",
    "        # initialize thetas\n",
    "        theta = np.ones(x.shape[1])\n",
    "\n",
    "        #iterate\n",
    "        for i in xrange(num_iter):\n",
    "            hypothesis = np.dot(x, theta)\n",
    "            loss = hypothesis - y\n",
    "            gradient = np.dot(x.transpose(), loss) / m\n",
    "            # update\n",
    "            theta = theta - alpha * gradient\n",
    "\n",
    "        self.theta = theta\n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent MSE:  0.583602464313\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model, metrics, cross_validation\n",
    "\n",
    "prostate = pd.read_csv(\"prostate.csv\")\n",
    "train, test = cross_validation.train_test_split(prostate, test_size=.33, random_state=42)\n",
    "selected_vars = ['age', 'lcp', 'pgg45', 'lpsa']\n",
    "dep_vars = [\"lcavol\"]\n",
    "test_reshaped = np.squeeze(test[dep_vars])\n",
    "train_reshaped = np.squeeze(train[dep_vars])\n",
    "\n",
    "gd = GradientDescent()\n",
    "gd.fit(train[selected_vars], train_reshaped, alpha = 0.0001, num_iter=100000)\n",
    "gd_MSE = metrics.mean_squared_error(gd.predict(test[selected_vars]), test_reshaped)\n",
    "print \"Gradient descent MSE: \", gd_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE from my gradient descent linear regression on the data from question 2.4 is 0.583602464313, which is close to and slightly larger than that of the normal linear regression using the selected variables, .566328775677."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (2+1+5+2 = 10 points)\n",
    "\n",
    "We will use Google's Tensorflow to create a simple multi-layered perceptron. Installation instructions can be found [here](https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#pip-installation). To make our lives even easier, we will be using [skflow](https://github.com/tensorflow/skflow). This can be installed via pip install skflow. This is a higher level API on top of tensorflow. You can find documentation on how to get started on the skflow page.\n",
    "\n",
    "To install tensorflow, this command should work (did on Mac):\n",
    "\n",
    "sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.0-py2-none-any.whl --ignore-installed\n",
    "\n",
    "1. Use pandas to get spam classification [data](https://archive.ics.uci.edu/ml/datasets/Spambase) from UCI. Don't worry about getting the column names. The last column is a 1 if spam, zero otherwise.\n",
    "2. Split the data into training and testing using test_size=0.33, random_state=42.\n",
    "3. Use a TensorFlowDNNClassifier to classify whether an email is spam and report your testing accuracy. You should use 1 hidden layer with 5 units, 50,000 steps, and a learning rate of .05. What does each parameter do and why does it matter?\n",
    "4. Compare your accuracy to a logistic regression using sklean. Discuss why one may have performed better than the other. You may also experiment with the architecture of your neural network (i.e. the number of hidden units, the number of nodes, the number of steps, and the learning rate) to see if you can improve your results from part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #1, avg. loss: 1.44190\n",
      "Step #5001, epoch #51, avg. loss: 0.66795\n",
      "Step #10001, epoch #103, avg. loss: 0.66545\n",
      "Step #15001, epoch #154, avg. loss: 0.66546\n",
      "Step #20001, epoch #206, avg. loss: 0.66537\n",
      "Step #25001, epoch #257, avg. loss: 0.66550\n",
      "Step #30001, epoch #309, avg. loss: 0.66555\n",
      "Step #35001, epoch #360, avg. loss: 0.66523\n",
      "Step #40001, epoch #412, avg. loss: 0.66527\n",
      "Step #45001, epoch #463, avg. loss: 0.66511\n",
      "Accuracy: 0.583278472679\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn import datasets, cross_validation, metrics, linear_model\n",
    "import tensorflow as tf\n",
    "\n",
    "import skflow\n",
    "\n",
    "#part 1\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', header=None)\n",
    "x = data.values[:,:57]\n",
    "y = data.values[:,57]\n",
    "\n",
    "#part 2\n",
    "train_x, test_x, train_y, test_y = cross_validation.train_test_split(x,y,test_size=0.33, random_state=42)\n",
    "\n",
    "#part 3\n",
    "classifier = skflow.TensorFlowDNNClassifier( hidden_units=[5,1], steps=50000, learning_rate=0.05, n_classes = 2)\n",
    "classifier.fit(train_x, train_y)\n",
    "result = classifier.predict(test_x)\n",
    "print (\"Accuracy: \" + str(metrics.accuracy_score(test_y,result)) + \"\\n\\n\")\n",
    "#print train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression accuracy: 0.930875576037\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.LogisticRegression()\n",
    "reg.fit(train_x, train_y)\n",
    "result = reg.predict(test_x)\n",
    "print \"Regression accuracy: \"+str(metrics.accuracy_score(test_y,result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (2+2+3 = 7 points)\n",
    "\n",
    "1. State briefly what you understand by the bias-variance tradeoff.\n",
    "\n",
    "2. For a given model and problem, what happens to these two quantities when the amount of training data available decreases, keeping all other factors remaining the same ( e.g. if 5-fold CV was used to train the original model, the same is used for the smaller dataset)?\n",
    "\n",
    "3. Suppose you want to approximate the pdf of a continuous random variable $X$, that takes on values over the interval (a,b), as follows: Get $N$ i.i.d samples of $X$; bin the interval into $k$ equi-spaced bins, and construct a histogram, which you then normalize so that total area under the histogram is 1. This normalized histogram will be an approximation of the true pdf. Clearly the histogram will change if you repeat this experiment using another $N$ samples; hence you can consider the quality of the solution in term of the 'mean' histogram (bias) and the variations among the histograms (variance).  Qualitatively explain how you would expect the bias-variance tradeoff to be reflected in this situation, as a function of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As model complexity increases, bias decreases and variance increases. This means that simpler/regularized models will be more stable with respect to variations in data, but will have more error on average; more complex/overfitted models will have lower average error and more sensitivity to variations in the data. The sweet spot is somewhere in between and can be achieved by tuning the amount of training data, type of model, and number of parameters.\n",
    "\n",
    "2. Variance increases; bias remains the same.\n",
    "\n",
    "3. With small k, the 'mean' histogram will be a rough approximation of the true pdf, i.e. high bias (pictorally: it is difficult to fill the area under a smooth curve with only 2 or 3 large rectangles), but there will be little variation among different data sets, as there are fewer bins for the samples to fall into. With large k, the 'mean' histogram will be a smoother/more accurate approximation of the true pdf, i.e. low bias, but will vary greatly among different data sets as the bins contain more precise intervals and may vary between having 0 and 1 samples, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
